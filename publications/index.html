<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Siddharth Singh | publications</title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">


    <span class="site-title">

        <strong>Siddharth</strong> Singh
    </span>


    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">blog</a>

        <!-- Pages -->








            <a class="page-link" href="/projects/">projects</a>



            <a class="page-link" href="/publications/">publications</a>



            <a class="page-link" href="/hobbies/">hobbies</a>







        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description">Publications in reversed chronological order.</h5>
  </header>

  <article class="post-content publications clearfix">
    <h4 id="accepted-papers">Accepted papers</h4>

<h3 class="year">2020</h3>
<ol class="bibliography"><li>

<div id="singh2020iclr">

    <span class="title">CURIOSITY INCREASES EQUALITY IN COMPETITIVE RESOURCE ALLOCATION</span>
    <span class="author">





                Bucher, Bernadette*,








              <em>Singh, Siddharth*</em>,








                Clelia de Mutalier,

                Kostas Daniilidis,

                Vijay Balasubramanian



    </span>

    <span class="periodical">

      <em>In Bridging AI and Cognitive Science (BAICS) workshop of The International Conference on Learning Representations</em>


      2020

    </span>


  <span class="links">

    [<a class="abstract">Abstract</a>]


    [<a href="https://baicsworkshop.github.io/pdf/BAICS_12.pdf" target="_blank">Link</a>]








  </span>

  <!-- Hidden abstract block -->

  <span class="abstract hidden">
    <p>We consider multiple agents using different strategies to compete for resources
with a diverse distribution of rewards. Statistical theory shows that two kinds of
equilibria are possible: (1) where some agents “settle” on a fixed resource while
others visit diverse sites, and (2) where all agents pursue a similar strategy of visiting diverse sites. The first equilibrium shows a highly skewed reward distribution;
in the second equilibrium most agents are similarly successful. We show that a
population of agents can learn these equilibrium strategies through reinforcement
learning. In conventional Q-learning, the population of agents learns the equilibrium strategy with skewed rewards. If we add curiosity, an intrinsic motivation
to explore, Q-learning converges faster, and produces the second equilibrium in
which most agents get similar average rewards. Thus, curiosity increases equality.</p>
</span>

</div>
</li></ol>

<h3 class="year">2019</h3>
<ol class="bibliography"><li>

<div id="dasari2019CoRL">

    <span class="title">RoboNet: Large-Scale Multi-Robot Learning</span>
    <span class="author">





              Dasari,Sudeep ,
              Ebert, Frederik ,
              Tian, Stephen ,
              Nair, Suraj ,
               Bucher, Bernadette ,
                Schmeckpeper, Karl ,
              <em>Singh, Siddharth</em>,
              Levine, Sergey ,
                Finn, Chelsea


    </span>

    <span class="periodical">

      <em>In Conference on Robot Learning (CoRL)</em>


      2019

    </span>


  <span class="links">

    [<a class="abstract">Abstract</a>]


    [<a href="https://arxiv.org/abs/1910.11215" target="_blank">Link</a>]

    [<a href="https://www.robonet.wiki/" target="_blank">Website</a>]




  </span>

  <!-- Hidden abstract block -->

  <span class="abstract hidden">
    <p>Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the promise of providing effective generalization to a wide range of open-world environments. However, these same methods typically require large amounts of diverse training data to generalize effectively. In contrast, most robotic learning experiments are small-scale, single-domain, and single-robot. This leads to a frequent tension in robotic learning: how can we learn generalizable robotic controllers without having to collect impractically large amounts of data for each separate experiment? In this paper, we propose RoboNet, an open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation. We combine the dataset with two different learning algorithms: visual foresight, which uses forward video prediction models, and supervised inverse models. Our experiments test the learned algorithms' ability to work across new objects, new tasks, new scenes, new camera viewpoints, new grippers, or even entirely new robots. In our final experiment, we find that by pre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka robot, we can exceed the performance of a robot-specific training approach that uses 4x-20x more data. For videos and data, see the project webpage: https://www.robonet.wiki/</p>
  </span>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/qd-sBiKGLn4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

</div>
</li>

<br>
<br>

<li>

<div id="Okelly2019Neurips">

    <span class="title">F1/10: An open-source 1/10th scale platform for autonomous racing and reinforcement learning</span>
    <span class="author">





              O,Kelly,Matthew ,
              Dhruv Karthik ,
              Hongrui Zheng ,
              Joseph Auckley ,
               <em>Siddharth Singh</em> ,
              Shashank D Prasad ,
              Kim Luong,
              Matt R Lebermann  ,
              Rahul Mangharam


    </span>

    <span class="periodical">

      <em>In Neural Information Processing Systems (Neurips)</em>


      2019

    </span>


  <span class="links">

    [<a class="abstract">Abstract</a>]


    [<a href="https://nips.cc/Conferences/2019/ScheduleMultitrack?event=15447" target="_blank">Link</a>]








  </span>

  <!-- Hidden abstract block -->

  <span class="abstract hidden">
    <p>The deployment of learning algorithms on autonomous vehicles is expensive, slow, and potentially unsafe. We present the F1/10 platform, a low-cost open-source 1/10th scale racecar and validated simulator which enables safe and rapid experimentation suitable for laboratory research settings. The F1/10 environment and hardware offer an accessible way to validate reinforcement learning policies learned in simulation and on a real car via the same intuitive OpenAI Gym API. To this effect, we demonstrate two approaches based on reinforcement learning that enable the transfer of policies from simulation to a real life track. The first demonstration details the use of our modular photorealistic simulator to learn an end-to-end racing policy in a self-supervised manner. Here, we demonstrate the F1/10’s capability for distributed online learning by sending batches of ‘experiences’ (video streams, odometry, etc.) back to a server, which asynchronously trains on this data and updates the racecar’s network weights on the fly. We also show a way to take quasi-deterministic ‘steps’ and perform ‘resets’ on the real car, thereby more closely mimicking the Gym API standards. The second demonstration uses our lightweight physics simulator to perform a joint optimization over a parameterized description of the racing trajectory, planning algorithms, and car dynamics, resulting in performance which exceeds all other entries in real-life races.</p>
  </span>

</div>
</li></ol>

<h3 class="year">2018</h3>
<ol class="bibliography"><li>

<div id="singh2019AIR">

    <span class="title">Gradient Aware - Shrinking Domain based Control Design for Reactive
Planning Frameworks used in Autonomous Vehicles</span>
    <span class="author">





      Adarsh Modh*
, <em>Siddharth Singh*</em>
, A. V. S. Sai Bhargav Kumar
, Sriram N. N.
, K. Madhava Krishna




    </span>

    <span class="periodical">

      <em>In Advances in Robotics (AIR)</em>


      2018

    </span>


  <span class="links">

    [<a class="abstract">Abstract</a>]


    [<a href="https://arxiv.org/pdf/1804.08679.pdf" target="_blank">Link</a>]








  </span>

  <!-- Hidden abstract block -->

  <span class="abstract hidden">
    <p>— In this paper, we present a novel control law for
longitudinal speed control of autonomous vehicles. The key
contributions of the proposed work include the design of a
control law that reactively integrates the longitudinal surface
gradient of road into its operation. In contrast to the existing
works, we found that integrating the path gradient into the
control framework improves the speed tracking efficacy. Since
the control law is implemented over a shrinking domain
scheme, it minimizes the integrated error by recomputing
the control inputs at every discretized step and consequently
provides less reaction time. This makes our control law
suitable for motion planning frameworks that are operating at
high frequencies. Furthermore, our work is implemented using
a generalized vehicle model and can be easily extended to
other classes of vehicles. The performance of gradient aware -
shrinking domain based controller is implemented and tested
on a stock electric vehicle on which a number of sensors
are mounted. Results from the tests show the robustness of
our control law for speed tracking on a terrain with varying
gradient while also considering stringent time constraints
imposed by the planning framework.</p>
  </span>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Yf4F0dvkwQE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</li></ol>


<h4 id="submitted-and-in-preparation">Submitted and in preparation</h4>

<h3 class="year">2021</h3>
<ol class="bibliography"><li>

<div id="Georg2021RSS">

    <span class="title">Learning to Map forActive Semantic Goal Navigation</span>
    <span class="author">




              Georgakis Georgios,
              Bernadette Bucher,
              <em> Siddharth Singh </em>,
              Karl Schmeckpeper,
              Kostas Daniilidis



    </span>

    <span class="periodical">

      <em> Submitted to the Robotics: Science and Systems Conference (RSS)</em>


      2021

    </span>


    <span class="links">

      [<a class="abstract">Abstract</a>]



    </span>
  <!-- Hidden abstract block -->
  <!-- Hidden abstract block -->

  <span class="abstract hidden">
    <p>— We consider the problem of object goal navigation inunseen environments. In our view, solving this problem requireslearning  of  contextual  semantic  priors,  a  challenging  endeavourgiven the spatial and semantic variability of indoor environments.Current methods learn to implicitly encode these priors throughgoal-oriented  navigation  policy  functions  operating  on  spatialrepresentations  that  are  limited  in  the  agent’s  observable  areas.In this work, we propose a novel framework that actively learnsto generate semantic maps outside the field of view of the agentand  leverages  the  uncertainty  over  the  semantic  classes  in  theunobserved areas to decide on long term goals. We demonstratethat through this spatial prediction strategy, we are able to learnsemantic  priors  in  scenes  that  can  be  leveraged  in  unknownenvironments. Additionally, we show how different objectives canbe  defined  by  balancing  exploration  with  exploitation  duringsearching   for   semantic   targets.   Our   method   is   validated   inthe  visually  realistic  environments  offered  by  the  Matterport3Ddataset  and  show  state  of  the  art  results  on  the  object  goalnavigation  task.</p>
  </span>

</div>
</li></ol>

  </article>





</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright of design to <a href = "https://github.com/yashpant/yashpant.github.io"> Yash Vardhan Pant</a>.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.


  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
